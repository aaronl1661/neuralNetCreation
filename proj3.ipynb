{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eOKrdDxAKSYC"},"outputs":[],"source":["#%% 2-2.5, 2.7\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","\n","\n","\n","train_images_np=np.load('./Project3_Data/MNIST_train_images.npy')\n","train_labels_np=np.load('./Project3_Data/MNIST_train_labels.npy')\n","val_images_np=np.load('./Project3_Data/MNIST_val_images.npy')\n","val_labels_np=np.load('./Project3_Data/MNIST_val_labels.npy')\n","test_images_np=np.load('./Project3_Data/MNIST_test_images.npy')\n","test_labels_np=np.load('./Project3_Data/MNIST_test_labels.npy')\n","\n","\n","##Template MLP code\n","def softmax(x):\n","    return np.exp(x)/np.sum(np.exp(x))\n","\n","def sigmoid(x):\n","    return 1/(1+np.exp(-x))\n","\n","def CrossEntropy(y_hat,y):\n","    return -np.dot(y,np.log(y_hat))\n","\n","class MLP():\n","\n","    def __init__(self):\n","        #Initialize all the parametres\n","        #Uncomment and complete the following lines\n","        self.W1 = np.random.normal(0,0.1,(64,train_images_np.shape[1]))\n","        self.b1 = np.zeros(64)\n","        self.W2 = np.random.normal(0,0.1,(10,64))\n","        self.b2 = np.zeros(10)\n","        self.reset_grad()\n","\n","    def reset_grad(self):\n","        self.W2_grad = 0\n","        self.b2_grad = 0\n","        self.W1_grad = 0\n","        self.b1_grad = 0\n","\n","    def forward(self, x):\n","        #Feed data through the network\n","        #Uncomment and complete the following lines\n","        self.x = x\n","        self.W1x = np.matmul(self.W1, self.x)\n","        self.a1 = self.W1x + self.b1\n","        self.f1 = sigmoid(self.a1)\n","        self.W2x = np.matmul(self.W2, self.f1)\n","        self.a2 = self.W2x + self.b2\n","        self.y_hat = softmax(self.a2)\n","\n","        return self.y_hat\n","\n","    def update_grad(self,y):\n","        # Compute the gradients for the current observation y and add it to the gradient estimate over the entire batch\n","        # Uncomment and complete the following lines\n","        dA2db2 = np.identity(10)\n","        # NOT the actual matrix, but we will store it like this and compute with it differently:\n","        # actual matrix is 10x640, this is the 1st 1x64\n","        dA2dW2 = self.f1\n","        dA2dF1 = self.W2\n","        dF1dA1 = np.diag(sigmoid(self.a1)*(1-sigmoid(self.a1)))\n","        dA1db1 = np.identity(64)\n","        # ALSO NOT the actual matrix, same as dA2dW2\n","        dA1dW1 = self.x\n","\n","        dLdA2 = self.y_hat - y\n","        # print(dLdA2.shape)\n","        # 1x10 * 10x640 = 1x640 --> unvectorize --> 10x64\n","        # is equivalent to 10x1 * 1x64\n","        dLdW2 = np.atleast_2d(dLdA2).T * dA2dW2\n","        dLdb2 = np.matmul(dLdA2, dA2db2)\n","        \n","        #these 2 gave me errors at first\n","        dLdF1 = np.matmul(dLdA2, dA2dF1)\n","        dLdA1 = np.matmul(dLdF1, dF1dA1)\n","        # same as dLdW2\n","        dLdW1 = np.atleast_2d(dLdA1).T * dA1dW1\n","        dLdb1 = np.matmul(dLdA1, dA1db1)\n","        self.W2_grad = self.W2_grad + dLdW2\n","        self.b2_grad = self.b2_grad + dLdb2\n","        # print(self.b2_grad.shape)\n","        self.W1_grad = self.W1_grad + dLdW1\n","        self.b1_grad = self.b1_grad + dLdb1\n","\n","    def update_params(self,learning_rate):\n","        self.W2 = self.W2 - learning_rate * self.W2_grad\n","        self.b2 = self.b2 - learning_rate * self.b2_grad.reshape(-1)\n","        self.W1 = self.W1 - learning_rate * self.W1_grad\n","        self.b1 = self.b1 - learning_rate * self.b1_grad.reshape(-1)\n","    \n","    def save(self, filename):\n","        np.savez(filename, self.W1, self.b1, self.W2, self.b2)\n","\n","    def load(self, filename):\n","        npzfile = np.load(filename)\n","        self.W1 = npzfile[\"arr_0\"]\n","        self.b1 = npzfile[\"arr_1\"]\n","        self.W2 = npzfile[\"arr_2\"]\n","        self.b2 = npzfile[\"arr_3\"]\n","\n","\n","## Init the MLP\n","myNet=MLP()\n","\n","\n","learning_rate=1e-3\n","n_epochs=100\n","\n","# variables I added\n","batch_size = 256\n","\n","# combine training images & their labels --> useful for randomly choosing indices\n","training_data = np.hstack((train_images_np,np.atleast_2d(train_labels_np).T))\n","n_training = train_images_np.shape[0]\n","# n_training = 2000\n","training_accuracy = np.zeros(n_epochs)\n","# loss per epoch\n","training_loss = np.zeros(n_epochs)\n","# loss per image per epoch (temporary)\n","training_losses = np.zeros(n_training)\n","\n","n_validation = val_images_np.shape[0]\n","\n","validation_accuracy = np.zeros(n_epochs)\n","validation_loss = np.zeros(n_epochs)\n","validation_losses = np.zeros(n_validation)\n","numCorrect_validation = 0\n","\n","## Training code\n","for iter in range(n_epochs):\n","    #Code to train network goes here\n","\n","    # Counter for # of correct image classifications this epoch\n","    numCorrect = 0\n","    # shuffle training data\n","    np.random.shuffle(training_data)\n","    # batch indices\n","    i_start, i_end = 0,0\n","\n","    # batch go thru all training data points: \n","    while i_end < n_training:\n","        \n","        # update batch indices\n","        i_start = i_end\n","        if i_end + batch_size < n_training:\n","            i_end += batch_size\n","        else:\n","            i_end = n_training\n","\n","        # reset gradients for each batch\n","        myNet.reset_grad()\n","\n","        # for each batch, go thru each data opint\n","        for image_index in range(i_start, i_end):\n","            # predict w/ forward pass\n","            myNet.forward(training_data[image_index, :-1])\n","            \n","            # track # correct - not necessary, but to check if gradient is minimizing correct thing\n","            i_hat = np.argmax(myNet.y_hat)\n","            i = training_data[image_index, -1]\n","            if i == i_hat:\n","                numCorrect += 1\n","            # track losses\n","            y = np.zeros(10)\n","            y[i] = 1\n","            training_losses[image_index] = CrossEntropy(myNet.y_hat, y)\n","\n","            # update: back prop\n","            myNet.update_grad(y)\n","\n","        # gradient update\n","        myNet.update_params(learning_rate)\n","\n","    training_accuracy[iter] = numCorrect / n_training\n","    training_loss[iter] = np.average(training_losses)\n","\n","\n","    #Code to compute validation loss/accuracy goes here\n","    numCorrect_validation = 0\n","\n","    for j in range(n_validation):\n","        # predict w/ forward pass\n","        myNet.forward(val_images_np[j])\n","        \n","        # check if it was a correct guess or not\n","        index_hat = np.argmax(myNet.y_hat)\n","        index = val_labels_np[j]\n","        if index == index_hat:\n","            numCorrect_validation += 1\n","        # track losses\n","        y = np.zeros(10)\n","        y[index] = 1\n","        validation_losses[j] = CrossEntropy(myNet.y_hat, y)\n","\n","    validation_accuracy[iter] = numCorrect_validation / n_validation\n","    validation_loss[iter] = np.average(validation_losses)\n","\n","\n","fig, ax = plt.subplots(2,1, figsize=(10,10))\n","\n","ax[0].plot(training_loss, label=\"Training\")\n","ax[0].plot(validation_loss, label=\"Validation\")\n","ax[0].set_title(\"Losses vs Epoch using all images\")\n","ax[0].set_ylabel(\"Average Cross Entropy Loss\")\n","ax[0].set_xlabel(\"Epoch Number\")\n","ax[0].legend()\n","\n","ax[1].plot(training_accuracy, label=\"Training\")\n","ax[1].plot(validation_accuracy, label=\"Validation\")\n","ax[1].set_title(\"Accuracies vs Epoch using all images\")\n","ax[1].set_ylabel(\"Model Accuracy\")\n","ax[1].set_xlabel(\"Epoch Number\")\n","ax[1].legend()\n","\n","plt.show()\n","\n","myNet.save(\"MLP_weights.npz\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ilhsxe33LQOv"},"outputs":[],"source":["#%% 2.6\n","\n","n_test = test_images_np.shape[0]\n","test_accuracy = 0\n","test_loss = 0\n","test_losses = np.zeros(n_test)\n","numCorrect_test = 0\n","\n","for i in range(n_test):\n","    # predict w/ forward pass\n","    myNet.forward(test_images_np[i])\n","    \n","    # check if it was a correct guess or not\n","    index_hat = np.argmax(myNet.y_hat)\n","    index = test_labels_np[i]\n","    if index == index_hat:\n","        numCorrect_test += 1\n","    # track losses\n","    y = np.zeros(10)\n","    y[index] = 1\n","    test_losses[i] = CrossEntropy(myNet.y_hat, y)\n","\n","test_accuracy = numCorrect_test / n_test\n","test_loss = np.average(test_losses)\n","\n","#output\n","print(\"Test accuracy: \" + str(test_accuracy))\n","print(\"Test loss: \" + str(test_loss))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_0vhGVccveD"},"outputs":[],"source":["#%% 2.8 Confusion Matrix\n","#use predictions variable + test_labels\n","confusion_matrix = np.zeros((10,10))\n","for i in range(n_test):\n","    confusion_matrix[test_labels_np[i], int(predictions[i])] += 1\n","\n","for row in range(10):\n","    confusion_matrix[row] = np.divide(confusion_matrix[row], np.count_nonzero(test_labels_np == row))\n","\n","plt.matshow(confusion_matrix)\n","plt.colorbar()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4r87sfb7hWww"},"outputs":[],"source":["#%% 2.9 Visualize W1\n","\n","fig, ax = plt.subplots(8,8, figsize=(10,10))\n","for i in range(64):\n","    ax[i//8, i % 8].matshow(myNet.W1[i].reshape((28,28)))\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVOkGXGxLSBB"},"outputs":[],"source":["#%% 3: CNN\n","\n","## Template for ConvNet Code\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class ConvNet(nn.Module):\n","    #From https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x.view(-1,1,28,28))))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 4 * 4)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","#Your training and testing code goes here\n","#Your training and testing code goes here\n","# The network first defines a 2d convolution with one channel in and 6 channels out with a kernel size of 5\n","# Then defines a max pool operation of kernel size 2 and stride of 2\n","# Then defines another 2d convolution filter with 16 distinct 6 x 5 x 5 filters\n","# Defines a fully connected layer 16 * 4 * 4 dim input, 120 dim output\n","# Then defines another linear layer with 120 dim input and 84 dim output\n","# Then defines the final layer of 84 dim input and 10 dim output\n","\n","# The network applies the max pool operation to a rectified linear unit function onto the convolutional layer of the x value that is reshaped into the dimensions (-1,1,28,28)\n","# Then the input is applied to another convolutional layer that is applied to a another relu function inside the max pool operation \n","# The input is then reshaped into (-1, 16 * 4 * 4)\n","# Then the input is applied to another relu function being applied by the first linear layer\n","# then the second linear layer is applied and another relu function\n","# then the output is saved in the third linear layer\n","\n","# copied from 1. Load and normalize CIFAR10 on https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-loss-function-and-optimizer \n","\n","train_images_np=np.load('./Project3_Data/MNIST_train_images.npy')\n","train_labels_np=np.load('./Project3_Data/MNIST_train_labels.npy')\n","val_images_np=np.load('./Project3_Data/MNIST_val_images.npy')\n","val_labels_np=np.load('./Project3_Data/MNIST_val_labels.npy')\n","test_images_np=np.load('./Project3_Data/MNIST_test_images.npy')\n","test_labels_np=np.load('./Project3_Data/MNIST_test_labels.npy')\n","\n","train_dat = []\n","for i, elt in enumerate(train_images_np):\n","    train_dat.append([elt.astype(np.float32), train_labels_np[i].astype(np.float32)])\n","trainloader = torch.utils.data.DataLoader(train_dat, batch_size = 256, shuffle = True)\n","\n","test_dat = []\n","for i, elt in enumerate(test_images_np):\n","    test_dat.append([elt.astype(np.float32), test_labels_np[i].astype(np.float32)])\n","testloader = torch.utils.data.DataLoader(test_dat, batch_size = 256, shuffle = True)\n","classes = (0,1,2,3,4,5,6,7,8,9)\n","\n","net = ConvNet()\n","\n","# copied from 3. Define a Loss function and optimizer on https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-loss-function-and-optimizer\n","import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","trainingAcc = []\n","testingAcc = []\n","# copied from 4. Train the network on https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-loss-function-and-optimizer\n","for epoch in range(100):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels.long())\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:    # print every 2000 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n","            running_loss = 0.0\n","\n","    # Copied from section 5 of https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-loss-function-and-optimizer\n","    dataiter = iter(testloader)\n","    images, labels = next(dataiter)\n","\n","    outputs = net(images)\n","\n","    _, predicted = torch.max(outputs, 1)\n","\n","    print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5f}'\n","                                for j in range(4)))\n","\n","    correct = 0\n","    total = 0\n","    # since we're not training, we don't need to calculate the gradients for our outputs\n","    with torch.no_grad():\n","        for data in trainloader:\n","            images, labels = data\n","            # calculate outputs by running images through the network\n","            outputs = net(images)\n","            # the class with the highest energy is what we choose as prediction\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n","    trainingAcc.append(100*correct // total)                            \n","    correct = 0\n","    total = 0\n","\n","    # since we're not training, we don't need to calculate the gradients for our outputs\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","            # calculate outputs by running images through the network\n","            outputs = net(images)\n","            # the class with the highest energy is what we choose as prediction\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n","    testingAcc.append(100*correct // total)\n","\n","y = [i for i in range(len(trainingAcc))]\n","plt.plot(y, trainingAcc)\n","plt.plot(y, testingAcc)\n","plt.show()\n","torch.save(net.state_dict(), \"./cnnweights.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eA7zUdoeeZdd"},"outputs":[],"source":["train_images_np=np.load('./Project3_Data/MNIST_train_images.npy')\n","train_labels_np=np.load('./Project3_Data/MNIST_train_labels.npy')\n","val_images_np=np.load('./Project3_Data/MNIST_val_images.npy')\n","val_labels_np=np.load('./Project3_Data/MNIST_val_labels.npy')\n","test_images_np=np.load('./Project3_Data/MNIST_test_images.npy')\n","test_labels_np=np.load('./Project3_Data/MNIST_test_labels.npy')\n","\n","train_dat = []\n","for i, elt in enumerate(train_images_np):\n","    train_dat.append([elt.astype(np.float32), train_labels_np[i].astype(np.float32)])\n","trainloader = torch.utils.data.DataLoader(train_dat, batch_size = 256, shuffle = True)\n","\n","test_dat = []\n","for i, elt in enumerate(test_images_np):\n","    test_dat.append([elt.astype(np.float32), test_labels_np[i].astype(np.float32)])\n","testloader = torch.utils.data.DataLoader(test_dat, batch_size = 256, shuffle = True)\n","classes = (0,1,2,3,4,5,6,7,8,9)\n","\n","cnn_state_dict = torch.load('./cnnweights.pt')\n","new_cnn = ConvNet()\n","new_cnn.load_state_dict(cnn_state_dict)\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        # calculate outputs by running images through the network\n","        outputs = new_cnn(images)\n","        # the class with the highest energy is what we choose as prediction\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
